{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization for Sparsity:  L1 Regularization (lasso)\n",
    "\n",
    "This exercise is based on the tensorflow [playground](https://playground.tensorflow.org) program (developed by google to teach machine learning principles).\n",
    "You'll experiment L1 regularization for a small, noisy training data set to perform 'supervised) binary classification. In this kind of setting, overfitting is a real concern. Fortunately, regularization might help.\n",
    "\n",
    "The input data are bivariate, this yields the two features $X_1$ and $X_2$. Feature crosses such that their product $X_1X_2$, their squared values $X_1^2$ and $X_2^2$, or their sinus are also included as input for the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task I\n",
    "\n",
    "Consider now this [tensorflow playground model](https://playground.tensorflow.org/#activation=linear&regularization=L2&batchSize=7&dataset=xor&regDataset=reg-plane&learningRate=0.01&regularizationRate=0.1&noise=30&networkShape=&seed=0.10080&showTestData=false&discretize=false&percTrainData=20&x=true&y=true&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=true&cosY=false&sinY=true&collectStats=false&problem=classification&initZero=false&hideText=false&playButton_hide=false&regularizationRate_hide=false&percTrainData_hide=false&numHiddenLayers_hide=true&noise_hide=false&problem_hide=true&regularization_hide=false&dataset_hide=false&activation_hide=true), the exercice  consists of five related demos. To simplify comparisons, run each demo in a separate tab.  Notice that the thicknesses of the lines connecting FEATURES and OUTPUT represent the relative weights (coefficients) of each feature.\n",
    "\n",
    "| Demo | Regularization Type | Regularization Rate (lambda) |\n",
    "|------|---------------------|------------------------------|\n",
    "| 1    |   $L_2$             |       0.1                    |\n",
    "| 2    |   $L_2$             |       0.3                    |\n",
    "| 3    |   $L_1$             |       0.1                    |\n",
    "| 4    |   $L_1$             |       0.3                    |\n",
    "| 5    |   $L_1$             |      experiment              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    " - How does switching from $L_2$ to $L_1$ regularization influence the delta between test loss and training loss?\n",
    " - How does switching from $L_2$ to $L_1$ regularization influence the learned weights?\n",
    " - How does increasing the $L_1$ regularization rate (lambda) influence the learned weights?\n",
    " - Why the $L_1$ penalty seems most appropriate than $L_2$ one for this problem?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task II\n",
    "\n",
    "We consider now this [neural net model](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.01&regularizationRate=0.01&noise=50&networkShape=8,8,8,8,8,8&seed=0.91875&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=true&cosY=false&sinY=true&collectStats=false&problem=classification&initZero=false&hideText=false&playButton_hide=false&regularizationRate_hide=false&percTrainData_hide=false&numHiddenLayers_hide=false&noise_hide=false&problem_hide=true&regularization_hide=false&dataset_hide=false&activation_hide=false) with six hidden layers with eight units (neurons) each. We choose a non-linear activation function, namely the `ReLU` $g(x) = \\max (0,x)$ which is a standard choice in deep learning ([why?](#Some-words-on-activation-functions-for-deep-neural-nets)).\n",
    "\n",
    "\n",
    "Run the model as given, without regularization, for at least 1000 epochs. We can adjust if necessary the learning rate (increase it to speed up the convergence, or decrease it to gain in stability).\n",
    "Note the following :\n",
    "- delta between Test and Training loss.\n",
    "- The learned weights for features and hidden units (neurons)\n",
    "\n",
    "Redo the same operation using now a L1 regularization, with the regularization rate as given, in a separate tab\n",
    "\n",
    "**Questions:**\n",
    "- How does introducing $L_1$ regularization\n",
    "  - influence the test loss and also the  delta between test loss and training loss?\n",
    "  - influence the learned weights?\n",
    "- What are the only features that are selected with $L_1$ regularization? Is it in agreement with the 'optimal' decision boundary for this data set? Are the hidden layers useful here?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some words on activation functions for deep neural nets\n",
    "\n",
    "(Deep) neural nets use nonlinear activation functions for the units, i.e. the neurons, of the hidden layers to define the output of these units given the input. Logistic function (sometimes called *sigmoid*) or other sigmoïdal functions as the hyperbolic tangent 'tanh' are typically used since the 90's and 2000's. \n",
    "However these functions appear to be badly adapted to some deep neural net architectures like convolutional networks.\n",
    "Adoption of the  rectified linear unit (ReLU) activation function in the 2010's may be considered one of the few milestones that now permit the routine development of very deep neural networks, for several reasons:\n",
    "\n",
    "1. **Counter the _vanishing gradient problem_**\n",
    "\n",
    "  A general problem with the logistic or hyperbolic tangent 'tanh' functions is that they saturate. For instance the logistic snap to 1.0 for large positive input and  snap to -1 for large negative input, and is only really sensitive to changes when the input is near 0. \n",
    "\n",
    "  Layers deep in large networks using these nonlinear activation functions fail to receive useful gradient information. Error is back propagated through the network from the outputs and used to update the weights. The amount of error decreases dramatically with each additional layer through which it is propagated, given the derivative of the chosen activation function. This is called the *vanishing gradient problem* and prevents deep networks from learning effectively. \n",
    "  \n",
    "  Because ReLU is piecewise linear, it preserves many of the properties that make linear models easy to optimize with gradient-based methods. In particular, it is linear for large positive values which prevent from the *vanishing gradient problem*. It also preserves many of the properties that make linear models generalize well. Yet, it is a nonlinear function as negative values are always output as zero: this allows one to obtain more flexible prediction rules than just linear ones. This yields universal function approximators.\n",
    "\n",
    "2. **Make computation cheaper!**\n",
    "\n",
    "    ReLU is very cheap to compute: no need for any multiplication or call of complex function (and the gradient is super simple: 1 for positive value and 0 for negative value). This is useful when the number of units is tens of millions or more in deep architectures!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
